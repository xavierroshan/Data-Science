1. Authentication in mysql: 

There are different authentication methods in mysql. whet is the configured method is availabel @  mysql> select user, host, plugin from mysql.user where user ='root';
In case of auth_socket password in not required, just do use the command :sudo mysql. Instead of checking for username and password mysql checks with linux you is the user
running mysql process and from that it can figure out the user, no need of entering password.
Accessing mysql: sudo mysql (for root user) mysql (for roshan)


2.1 Working with Kafka in the k3s:

view kafka pods in k3s cluster:
sudo k3s kubectl get pods -n kafka

view kafka services in k3s cluster:
sudo k3s kubectl get services -n kafka

Get into kafka pod:
sudo k3s kubectl exec -it my-kafka-cluster-kafka-0 -n kafka -- bash



creat a topics in the pod:
/opt/kafka/bin/kafka-topics.sh --create --topic test-topic-3 --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

produce a message:
/opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic-3


consume a message:
/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic-3 --from-beginning





2. Accessing kafka from local machine (creating topics, producer and consumer)
creating - topic
export CLASSPATH="$PWD/libs/*"
./kafka-console-consumer.sh --bootstrap-server 192.168.2.48:30232 --topic test-topic-4 --from-beginning


Kafka message producer in local machine:
export CLASSPATH="$PWD/libs/*"
./kafka-console-producer.sh --broker-list 192.168.2.48:30232 --topic test-topic




Kafka message consumer:
cd ~/kafka-clients
export CLASSPATH="$PWD/libs/*"
./kafka-console-consumer.sh --bootstrap-server 192.168.2.48:30232 --topic test-topic --from-beginning


2. Accessing spark from 

view spark pods:
sudo kubectl get pods -n spark

submit the spark job from within the pod. using single command to get in and execute:

kubectl exec -it spark-master-0 -n spark -- /opt/bitnami/spark/bin/spark-submit --master spark://spark-master-svc.spark.svc.cluster.local:7077 /tmp/hello_world.py

submit the spark job from within the pod. using two command to get in and execute:
kubectl exec -it spark-master-0 -n spark -- /bin/bash
/opt/bitnami/spark/bin/spark-submit --master spark://spark-master-svc.spark.svc.cluster.local:7077 /tmp/hello_world.py

3. Accessing redis in docker

run docker instal with redis: docker run --name redis-container -p 6379:6379 redis
start redis: docker start redis-container
access and perform operations on redis: docker exec -it redis-container redis-cli


4. Accesssing mongodb in docker

start docker with dockerid: docker start 7509d1673325
Accessing the instance: mongosh "mongodb://localhost:27017"


5. Kafka in docker:

Accessing Kafka in docker: docker exec -it kafka kafka-topics --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1


Whats installed in Docker:

roshan@roshan:~$ docker ps
CONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS          PORTS                                                             NAMES
4ee61305dd6c   confluentinc/cp-kafka:latest       "/etc/confluent/dock…"   8 minutes ago   Up 8 minutes    0.0.0.0:9092->9092/tcp, [::]:9092->9092/tcp                       kafka
28962d65f395   confluentinc/cp-zookeeper:latest   "/etc/confluent/dock…"   9 minutes ago   Up 9 minutes    2888/tcp, 0.0.0.0:2181->2181/tcp, [::]:2181->2181/tcp, 3888/tcp   zookeeper
7509d1673325   mongo                              "docker-entrypoint.s…"   31 hours ago    Up 31 hours     0.0.0.0:27017->27017/tcp, [::]:27017->27017/tcp                   mongodb-container
a82584a35163   redis                              "docker-entrypoint.s…"   32 hours ago    Up 19 minutes   0.0.0.0:6379->6379/tcp, [::]:6379->6379/tcp                       redis-container
roshan@roshan:~$ 


